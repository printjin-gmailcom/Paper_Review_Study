# **멀티모달 논문 스터디**

## **개요**

이 레포지토리는 **2025년 7월 1일**부터 진행된 멀티모달 관련 논문 리뷰 스터디의 자료를 정리한 것입니다.

## **주요 특징**

- 멀티모달 학습의 기초 논문 리뷰
- 비전-언어 모델 등 다양한 멀티모달 아키텍처 분석

## Paper List

| Date | Paper | Topic | Links |
| --- | --- | --- | --- |
| 2025.7.1 | Attention Is All You Need | Transformer, Self-Attention | [paper](https://arxiv.org/abs/1706.03762) |
| 2025.7.6 | Learning Transferable Visual Models From Natural Language Supervision | Vision-Language Pretraining, CLIP | [paper](https://arxiv.org/abs/2103.00020) |
| 2025.7.13 | Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | Vision-Language Pretraining, BLIP | [paper](https://arxiv.org/abs/2201.12086) |
| 2025.7.13 | Align before Fuse: Vision and Language Representation Learning with Momentum Distillation | Vision-Language Pretraining | [paper](https://arxiv.org/abs/2107.07651) |
| 2025.7.20 | GIT: A Generative Image-to-text Transformer for Vision and Language | Vision-Language, Image Captioning | [paper](https://arxiv.org/abs/2205.14100) |
| 2025.7.27 | Flamingo: a Visual Language Model for Few-Shot Learning | Vision-Language, Few-Shot Learning | [paper](https://arxiv.org/abs/2204.14198) |
| 2025.8.3 | Visual Instruction Tuning | Vision-Language, Instruction Tuning | [paper](https://arxiv.org/pdf/2304.08485) |
| 2025.8.10 | InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning | Vision-Language, Instruction Pretraining | [paper](https://arxiv.org/abs/2305.06500) |
| 2025.8.17 | Osprey: Pixel Understanding with Visual Instruction Tuning | Vision-Language, Pixel-Level Instruction | [paper](https://arxiv.org/pdf/2312.10032) |
| 2025.8.24 | Vision-Language Instruction Tuning: A Review and Analysis | Vision-Language, Instruction Review | [paper](https://arxiv.org/abs/2311.08172) |
| 2025.8.31 | BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions | Multimodal LLM, Text-Rich VQA | [paper](https://arxiv.org/abs/2308.09936) |
| 2025.9.7  | VATT: Video-Audio-Text Transformer | Multimodal, Video-Audio-Text | [paper](https://arxiv.org/abs/2104.11178) |
| 2025.9.14 | AV-HuBERT: Self-Supervised Audio-Visual Speech Representation Learning | Multimodal, Audio-Visual Speech | [paper](https://arxiv.org/abs/2201.02184) |
| 2025.9.21 | Look, Listen and Learn | Multimodal, Audio-Visual Representation Learning | [paper](https://arxiv.org/abs/1705.08168) |
| 2025.9.28 | The “something something” video database for learning and evaluating visual common sense | Dataset, Video Understanding | [paper](https://arxiv.org/pdf/1706.04261) |
| 2025.10.05 | Ego4o: Egocentric Human Motion Capture and Understanding from Multi-Modal Input         | Egocentric Vision, Human Motion, Multi-Modal          | [paper](https://arxiv.org/abs/2504.08449) |
| 2025.10.12 | UniSpeech-SAT: Universal Speech Representation Learning with Speaker-Aware Pre-Training | Speech Representation, Self-Supervised, Speaker-Aware | [paper](https://arxiv.org/pdf/2110.05752) |



