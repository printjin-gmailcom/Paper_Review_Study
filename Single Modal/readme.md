# **싱글모달 논문 스터디**

## **개요**

이 레포지토리는 **2025년 7월 1일**부터 진행된 싱글모달 관련 논문 리뷰 스터디 자료를 정리한 것입니다.

## **주요 특징**

* NLP, Vision, Voice 등 싱글모달 학습 관련 기초 및 최신 논문 리뷰
* Word Embedding, Transformer, Seq2Seq, Contextual Representation, CNN 등 다양한 싱글모달 아키텍처 분석

## Paper List

| 날짜 | 논문 제목 | 주제 | 링크 |
| --------- | --------------------------------------------------------------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------- |
| 2025.7.5  | Attention Is All You Need | Transformer, Self-Attention | [paper](https://arxiv.org/abs/1706.03762) |
| 2025.7.12 | Efficient Estimation of Word Representations in Vector Space | Word Embedding, Skip-gram | [paper](https://arxiv.org/abs/1301.3781) |
| 2025.7.19 | Glove: Global Vectors for Word Representation | Word Embedding, Co-occurrence Matrix | [paper](https://www.researchgate.net/publication/284576917_Glove_Global_Vectors_for_Word_Representation) |
| 2025.7.26 | Sequence to Sequence Learning with Neural Networks | Seq2Seq, Encoder-Decoder | [paper](https://arxiv.org/pdf/1409.3215) |
| 2025.8.2  | Deep contextualized word representations | Contextual Word Embedding, ELMo | [paper](https://arxiv.org/abs/1802.05365) |
| 2025.8.9  | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  | Language Representation, BERT | [paper](https://arxiv.org/pdf/1810.04805) |
| 2025.8.16 | Language Models are Few-Shot Learners | GPT-3, Few-Shot Learning | [paper](https://arxiv.org/pdf/2005.14165) |
| 2025.8.30 | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | T5, Text-to-Text Transfer Learning | [paper](https://arxiv.org/abs/1910.10683) |
| 2025.9.13 | ImageNet Classification with Deep Convolutional Neural Networks (AlexNet) | CNN, Image Classification | [paper](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) |
| 2025.9.20 | Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG16) | CNN, Deep Learning | [paper](https://arxiv.org/abs/1409.1556) |
| 2025.9.27 | Going Deeper with Convolutions | CNN, Inception | [paper](https://arxiv.org/pdf/1409.4842) |
| 2025.10.04 | Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks | Object Detection, RPN, CNN| [paper](https://arxiv.org/pdf/1506.01497) |
| 2025.10.11 | End-to-End Object Detection with Transformers| Object Detection, Transformer, DETR | [paper](https://arxiv.org/abs/2005.12872)|
| 2025.10.18 | Deep Speech: Scaling up end-to-end speech recognition| Speech Recognition, RNN, CTC, End-to-End | [paper](https://arxiv.org/abs/1412.5567) |
| 2025.10.25 | Deep Speech 2: End-to-End Speech Recognition in English and Mandarin | Speech Recognition, RNN, BatchNorm, End-to-End | [paper](https://arxiv.org/abs/1512.02595) |
| 2025.11.08 | Is Space-Time Attention All You Need for Video Understanding? | Video Understanding, Transformer, Attention, TimeSformer | [paper](https://arxiv.org/abs/2102.05095) |
| 2025.11.15 | WAVENET : A GENERATIVE MODEL FOR RAW AUDIO | Generative Model, Raw Audio, Autoregressive, Dilated Convolution | [paper](https://arxiv.org/pdf/1609.03499) |
| 2025.11.22 | Tacotron : Towards End-to-End Speech Synthesis | End-to-End TTS, Seq2Seq, Attention | [paper](https://arxiv.org/abs/1703.10135) |
| 2025.11.29 | Deep Speech : Scaling up end to end speech recognition | End-to-End ASR, RNN, CTC | [paper](https://arxiv.org/abs/1412.5567) |
| 2025.12.06 | HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units | Self-Supervised ASR / Speech Representation | [paper](https://arxiv.org/abs/2106.07447) |
| 2026.01.07 | DEBERTAV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing | NLP, Transformer, Pretraining | [paper](https://arxiv.org/abs/2111.09543)
| 2026.01.14 | RoBERTa: A Robustly Optimized BERT Pretraining Approach | NLP, BERT, Pretraining | [paper](https://arxiv.org/abs/1907.11692)
| 2026.01.21 | TRIBE: TRImodal Brain Encoder for whole‑brain fMRI response prediction | Brain Encoding, Multimodal, fMRI | [paper](https://arxiv.org/abs/2507.22229)
| 2026.01.28 | XLNet: Generalized Autoregressive Pretraining for Language Understanding | NLP, Transformer, Autoregressive | [paper](https://arxiv.org/abs/1906.08237)
| 2026.02.04 | FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | NLP, Transformer, Attention Optimization | [paper](https://arxiv.org/pdf/2205.14135) |
| 2026.02.11 | A ConvNet for the 2020s | CV, ConvNet Architecture | [paper](https://arxiv.org/pdf/2201.03545) |
